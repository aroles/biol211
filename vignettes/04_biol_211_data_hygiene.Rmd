---
title: "Data Collection"
author:
    - "Jason M. Gleditsch ^[Temple University, jmg5214@gmail.com]"
    - "Sebastiano De Bona ^[Temple University, sebastiano.debona@gmail.com]"
    - "Mathew R. Helmus ^[Temple University, mrhelmus@temple.edu]"
    - "Jocelyn E. Behm ^[Temple University, jebehm@temple.edu]"
date: "Last Updated:  `r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
  pdf_document:
    toc: TRUE
    toc_depth: 2
---

# Summary

This section is aimed at providing guidelines for the treatment of data while and after it is collected. During this period, data is the most vulnerable to loss or other issues that decrease its usefulness. Therefore, proper data management during this period is absolutely essential!

----

# Data Collection



## Field Data

* **Data should be recorded with dark pen and never erased or scratched out**

When collecting data, a dark inked pen should be used. This ensures that information will not be lost due to pencil lead being rubbed off. Additionally, pen forces changes made to the data in the field to be tracked on the page (i.e. you cannot erase). If a mistake is made, the mistake should be crossed out with a single line and the initials of the person making the change written near the change. Information should NEVER be scratched out so much that the information being changed cannot be recovered.

If you are worried about rain or water damage to your data sheets, then you should use All-Weather paper and an archival or All-Weather pen. However, these types of pens can be expensive, so a pencil can be used when necessary (with the permission of the PIs or project lead). If a pencil is used, the data recorder needs to press hard enough that the pencil writes as dark as possible. The same guidelines for making changes in the field should be followed so that the changes can be tracked.

After each day of data collection in the field, the data sheets should be legibly photographed or scanned and saved to your project Drive Folder. This provides the most basic back up that you can go to if data is lost.

For audio-visual data, information about the data, such as the date, time, study name, person taking the recording and any other information necessary for the independent comprehension of the data, should be included as a message in the frame for images or at the start of video and audio recording. This makes the file self-identifying in case that information is lost elsewhere. For remotely collected audio-visual data, this can be done immediately after the data is collected (see the [Data Formatting](#data-formatting) section).


## Lab Data

* **Data collected in the lab should kept together in a lab notebook and recorded in dark ink**

Data collected in a lab should be collected in Lab Notebooks. Lab Notebooks can be actual notebooks and will be provided by the PI (please ask if they are not readily available). Binders should be avoided when possible in lieu of bound notebooks because pages area easily removed from or fall out of binders. If you a collecting data for the development of a product for patenting, then you are required to use a bound notebook and to review the institution’s policy on lab notebooks. Since ecological research rarely produces patented products, we will not give guidelines for keeping a legal lab notebook. However, if you want more information on this, then it is recommended you review “Writing the Laboratory Notebook” by Howard Kanares ([LINK](https://files.eric.ed.gov/fulltext/ED344734.pdf)).

Key points in keeping a Lab Notebook:

1.	Neat and legible handwriting in dark ink; not pencil if able

2.	Procedure/Study title and purpose clearly stated

3.	Methods described clearly and succinctly, with errors and steps taken to correct them

4.	Calculations performed neatly showing intermediate steps

5.	Errors crossed out with a single line, initialed, and briefly explained

6.	All pages dated at the top and numbered at the bottom

When making a Lab Notebook, make sure to leave enough room at the beginning for a table of contents. Every new procedure, experiment, notes, calculation, etc. should start on a new page with that page being the front of the right page (i.e. the odd-numbered pages). These new pages will be what is recorded in the table of contents for each procedure, experiments, set of notes, etc. For the sake of being able to easily find a specific date or page number, the date should be recorded at the right-hand side of the top of the page and the page number should be written at the right-hand side of the bottom of the page.

All information about a procedure, experiment, notes, calculation, etc. should be recorded in the Lab Notebook. This includes data recorded on or collected with a computer system. The data should be printed out and taped (never glued) into the Lab Notebook on the appropriate page. These entries should be accompanied by a brief description of what it is. If the data collected by a computer is too large to print and tape into the notebook then the name of the data file and where it is located should be recorded along with the location of any backups/copies made.

As with the collection of field data, information in a Lab Notebook should never be completely removed from the book. Mistakes, such as misspellings, should be crossed out with a single line and initialed. It is also good practice to give a short (only a few words) reason the change was made.

Lab Notebooks should **NEVER** be taken out of the lab or the project lead’s possession. Ideally, Lab Notebooks should be kept in a cabinet/drawer in the lab space so that collaborators can easily find them. However, it is also acceptable for lab notebooks to be kept in the office of the project lead. To protect against Lab Notebooks being removed from the lab, digital scans or photographs of the notebook’s pages should be created periodically (ideally weekly). This allows for the consultation of the Lab Notebook when not in the lab and acts as a backup of the information in the notebook.


## Data Transcription

* **Digitize and back up data soon after collection**

Ideally, the data should be transcribed into a computer each day after collection as well. If that is not possible, then data should be transcribed weekly. This makes transcribing data easier and reduces the chances for transcription errors for multiple reasons. First, data will not pile up, and when data piles up and transcribed all at once, errors are more likely due to rushing and fatigue. Lastly, regular data transcription helps for understanding messy handwriting in the field. The data will still be fresh in the mind of the researcher and any temporary technicians will still be around, both of which are important for deciphering handwriting.

If multiple people are transcribing data, then they should each be working with separate but identically formatted spreadsheets. This reduces the risk of someone accidentally overwriting or deleting data. It is then the responsibility of the data manager or project lead to compile this data in a reproducible way (e.g. with R script or detailed methodology). The compilation of data can occur whenever the data manager deems it necessary. 


$$~$$


## Data Proofing

* **A secondary check by someone who did not enter the data should be done**

After data has been entered, someone who did not enter the data should compare the hand-written data sheet(s) to the entered data. When doing this, there should be a proofing column(s) in the data file that gets checked off once the data has been checked, and has notes for corrections and/edits that were made as well as a column for the reviewer’s names or initials. Again this is most important for when there are many researchers entering data but will always help to catch and correct any errors early. If there is only one researcher entering data, then having that researcher go back and recheck the data they have already entered at a later date (e.g. a week later) is a easy way to do data proofing even on a small project. 

If there is too much data for the data proofing stage to be reasonably done, then a random subset of the data entered by each data transcriber should be proofed.

Further proofing will also be done during data formatting (see the [Data Formatting](#data-formatting) section) and should be always be done before analysis.

----

# Data Formatting

This section provides the guidelines for how to build and format data tables so that they are easily manipulated, edited, analyzed, and most importantly understood. In addition, we will discuss the proper formatting of other types of data as well such as spatial and audio-visual data. The formatting outlined in this section should be established well before the final version of the data is made meaning that the early files that are being manipulated/edited do not necessarily have to be formatted this way. However, it is highly recommended that the data (especially data tables) be formatted following these guidelines during the first edit (i.e. the edit(s) creating version 1).

## Data Tables

* **Data tables should be saved as CSV files and be in long format**

* **Column headers are required, should be consistent, and easy to understand**
           
* **All text should be formatted consistently with dates in the YYYY-MM-DD format**

* **All cells should have values and if a value is missing then an NA should be entered**

All data tables should be saved as a comma delimited file (never tab delimited) such as a CSV (comma separated values) file. The reason for this is that Excel files may not be able to be easily read by researchers without Microsoft Office. This is also the same for other proprietary file types such as Access. If you are using a relational database software, such as Access, then the various tables should also be saved as CSV files. This is also true for data stored in R file types (e.g. Rdata), because not everyone knows how to use R.

Even though CSV and comma delimited text files are essentially the same, CSV files are the preferred format for data tables over text files because the .csv extension provides an easy way to sort or filter out the data files since the documentation files (see the File Documentation section) are often saved as text file (.txt extension). Additionally, CSV files are readable by most software and easy to load into most computer programs.

Data tables should always be in a long format unless a long format is too cumbersome (e.g. large community data). A long-formatted data table is a table where one row corresponds to the minimum observable unit of data (e.g. a single trial for an individual in a repeated behavioral assay) that has multiple rows for grouping factors (e.g. individual) and single columns for data (e.g. choice). However, even if long formats seem too cumbersome, long formats are better than wide for many reasons. Specifically, long formats allow for the easy conversion to other formats. Long formats also allow for the easy aggregation, subsetting, and analysis. 

Below is an example of data in a long format:

```{r tab, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE}
tab <- data.frame(Date = rep('04/24/2020', 12),
                  Site = c(rep('A', 6), rep('B', 6)),
                  Species = c('Grackle', 'Grackle', 'House Finch', 'House Finch', 'American Robin', 'American Robin',
                              'Grackle', 'Grackle', 'House Finch', 'House Finch', 'American Robin', 'American Robin'),
                  Sex = rep(c('M', 'F'), 6),
                  Count = c(2, 1, 7, 9, 4, 3, 0, 2, 10, 15, 4, 6))

knitr::kable(tab)
```

Every data table is REQUIRED to have column headers. Column headers should be short and easy to understand. Avoid using symbols and abbreviations whenever possible. For example, instead of using ‘dc’ to denote the distance to Cuba, use ‘distance_cuba’. Abbreviations are helpful, especially during analysis coding, but can lead to difficulties in comprehension. Make sure that if you use abbreviations you use common, easy to understand abbreviations. For instance, instead of ‘distance_cuba’ we could use ‘dist_cuba’ since “dist” is an often-used abbreviation for distance. Here at the iEco Lab we prefer you to not use abbreviations except for instances when the column header is very long (a general guideline is that headers should be no more than 15 characters and therefore ‘distance_cuba’ is not too long). Additionally, for column headers, spaces should be replaced with an underscore (i.e. “_”). Some analysis software and R functions cannot handle spaces in column headers, which can cause the data to erroneously load or return errors. Column header conventions should be made for individual projects with multiple data tables or for whole labs. Column header conventions help to make data easily combined for analysis and understood. 

Column headers also should not include units. The units of measurement should be included in you meta data for that file. If you have multiple columns that have the same data but different units that are easy to convert (e.g. m and km) then delete one of those columns and convert in your analysis code if needed. The only exception to not having units in the header is if you have the same data with different units that are not easily converted between each other and are on very different scales (e.g. atmospheric pressure, temperature), or if for some reason you need both metric and imperial units (e.g. for making maps for US and non-US users).

The format of any text (including numbers) in your data table should be as basic as possible. The file types that should be used (i.e. csv or text files) strip all formatting from any text. This can cause issues if you use special characters, symbols, and fonts. Therefore, all special characters and symbols should be avoided. Additionally, most analysis software do not know how to read special characters or symbols. 

Here is a list of acceptable symbols and the top 10 worst symbols:

```{r symbols, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE}
acc <- data.frame(Name = c('Underscore', 'Dash', 'Veritcle Line', 'Semicolon', 'Period'),
                  Symbol = c('_', '-', '|', ';', '.'))

bad <- data.frame(Name = c('Dollar Sign', 'Question Mark', 'Percen Sign', 'Number Sign', 'Commercial At', 'Comma',
                           'Colon', 'Backslash (Reverse Solidus)', 'Quotation Mark', 'Apostrophe'),
                  Symbol = c('$', '?', '%', '#', '@', ',', ':', '\\', '"', "'"))
```

$\underline{Acceptable \space Symbols:}$

```{r acc, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE}
knitr::kable(acc)
```

$\underline{Symbols \space to \space Avoid \space at \space All \space Cost:}$

```{r bad, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE}
knitr::kable(bad)
```

For text strings (column headers and factors with more than 1 character) avoid using capitalized letters where possible. If you do use capital letters, make sure you are consistent with how you use them. For example, if you have binomial species names and you capitalize the genus but not the species, then make sure you use that same framework throughout. In the above example the first letter of each word in the species name is capitalized throughout that column. However, a good rule of thumb to follow is to never use capitalization where possible.

Entry IDs should be included in all tables you produce. Entry IDs are unique identifiers for each entry (i.e. row) in a data table. This allows for easy tracking of specific changes you make to the data as well as allowing for the easy creation of relational keys between your data tables for easy merging. Avoid just using simple row numbers for these IDs. Good entry IDs are alpha-numeric text strings that provides some information about the entry, allows for easy sorting, and should be the same number of characters for each entry. These IDs should not be typed in with the data and be simple (few numbers and characters) or complex. It is often easiest to make IDs by concatenating multiple columns together with a separating symbol.

For example, a good entry ID for the first entry in the above table would be “20200424_A_02_M” which tells us that entry is for site A, the second species alphabetically, is male, and was made on a particular survey day. We could also use that ID to sort so that the sites and species within those sites are alphabetical from the earliest survey to the last. 

Entry IDs should also be unique to the data table meaning that the ID scheme should not be repeated across tables that may be later joined. The entry ID should be created at some point before the final version of the data is created. The timing at which the entry ID is made depends on the specific needs of the project and data editing. Once the entry ID is made then it is set for the entire lifetime of the data and should not be change. However, it is recommended to create the entry ID after all of the large edits to the data are made. This is because if large sections of data are deleted then you will have large gaps in the entry ID series. Do not worry about adding data after an entry ID is created since it is easy to just continue the series where you left off. Importantly, entry IDs should never be recycled even if an entry has been deleted. This could make following the change logs difficult.

Dates in data tables should be in the format of YYYY-MM-DD, and no other format should be used. In the above table the format used for the date is MM/DD/YYYY. This format is bad for multiple reasons. First, not every culture and, therefore, their computers use that format. In many regions, the day comes before the month (i.e. DD/MM/YYYY). This can cause issues for dates like 4/5/2020. Is that April 5th or is it May 4th? Computers in different cultures will also ask that question and may just assume that the first number is the day or month depending on its default setting. The year-month-day format with dashes in between the year, month, and day is understood by all computers and software. Additionally, by having dates in the YYYY-MM-DD you can easily sort your data by date. When including years in dates they always should be 4 digits so that 2020 is not confused with 1920. If you like having the character form of months (e.g. March or Mar) in your data tables, then you should have that separate from the date in a new column.

Missing data should be handled the same throughout the data table. This includes any comment columns you have in the table. **NEVER** leave cells blank for missing data. Blank cells have an ambiguous meaning since they could mean that there was no data for that cell, the cell was accidentally deleted, or it was erroneously skipped over during transcription. Instead, use something like ‘NA’ to denote missing data. It is preferred to use ‘NA’ to denote missing data since it is read by R software in that way. No matter what is the cause of the missing data (e.g. data not recorded, data not able to be recorded, etc.), missing data should always by represented by an NA. If it is important for the project to differentiate between different causes of NAs then this should be recorded in a ‘notes’ or ‘comments’ column. If a specific value is wanted to denote a certain cause of missing data (e.g. NR for data not recorded), then that should be a decision made by the PI and Project Lead. However, be careful when reading this data into statistical software since they will not know what that value (e.g. NR) means and will likely read it as a character making the whole column a character variable.

When creating data tables, the most important thing is the data to be consistent and the nature of the data recorded in the meta data file (see File Documentation section). For example, species names should always be either only common or only scientific in a single column, and in comment columns, if a comment means the same thing, it should be kept the same (e.g. "date not recorded" is always entered, and never "date was not recorded", "date missed", "unrecorded date" and so on). Further, in comments DO NOT use commas to separate clauses. This is because in CSV files, commas are used to separate each field (i.e. column).

Lastly, make sure that there are no leading or trailing spaces in your data. This can cause some software to mess up your data or treat text strings as separate levels of a factor even if they are identical except for a trailing or leading space.

Now if we rework the above example table into the ideal formatting it will look like:

```{r tab1, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE}
tab$Date <- '2020-04-24'
tab$Site <- tolower(tab$Site)
tab$Species <- tolower(tab$Species)
tab$Sex <- tolower(tab$Sex)
tab$entry_id <- paste(gsub("-", "", tab$Date, fixed = TRUE), tab$Site, paste0('0', as.numeric(as.factor(tab$Species))),
                      tab$Sex, sep = '_')

tab <- tab[, c('entry_id', 'Date', 'Site', 'Species', 'Sex', 'Count')]
names(tab) <- c('entry_id', 'date', 'site', 'common', 'sex', 'count')


knitr::kable(tab)
```

Now this table does not have any uppercase letters, has an informative alpha-numeric entry ID, the data is formatted correctly, and the headers are appropriate (i.e. 'Species' was changed to 'common' and there are no uppercase letters).

----
